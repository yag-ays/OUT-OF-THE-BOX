<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Out-of-the-box</title>
    <link>https://yag-ays.github.io/</link>
    <description>Recent content on Out-of-the-box</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 27 Feb 2019 22:36:40 +0900</lastBuildDate>
    
	<atom:link href="https://yag-ays.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>単語埋め込みにおけるout-of-vocabularyの対応 - magnitudeの初期化</title>
      <link>https://yag-ays.github.io/project/out-of-vocab-magnitude/</link>
      <pubDate>Wed, 27 Feb 2019 22:36:40 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/out-of-vocab-magnitude/</guid>
      <description>概要 magnitudeという単語埋め込みを扱うライブラリには、単語を構成する文字列を考慮したout-of-vocabularyの初期化の方法が実装されています。EMNLP 2018の論文と実際のコードを元に、その初期化の方法を実装して試してみました。
背景 KaggleのQuora Insincere Questionsコンペを終えて KaggleのQuora Insecure QuestionsのコンペではOOVの対応が重要だったっぽいけど、magnitudeはランダムベクトルの付与とかミススペルの対応とかしてくれるみたいだ。ロジック確認しないと何してるのかわからないけど……　https://t.co/d8tteqwwCp
&amp;mdash; やぐ (@yag_ays) February 26, 2019 
KaggleのNLPコンペであるQuora Insincere Questions Classificationが終わって上位陣の解法を眺めていたのですが、その中で目に止まったのがout-of-vocabulary（以降OOVと表記）の対応です。今回のコンペでは主催側が定めた幾つかの学習済み単語埋め込みしか使うことができないので、大規模コーパスから新しく学習することができません。そのため、データセットには含まれているが学習済み単語ベクトルには含まれていない単語であるout-of-vocabularyをどう扱うかが、前処理の重要な要素だったようです。それぞれの解法には以下のようなコメントが記載されています。
 &amp;ldquo;The most important thing now is to find as many embeddings as possible for our vocabulary. (中略) For public test data we had around 50k of vocab tokens we did not find in the embeddings afterwards. &amp;rdquo; (1st place solution) &amp;ldquo;I applied spell correction to OOV words.</description>
    </item>
    
    <item>
      <title>後処理のみで単語ベクトルの性能を向上させるALL-BUT-THE-TOPを使った日本語学習済み分散表現</title>
      <link>https://yag-ays.github.io/project/all-but-the-top/</link>
      <pubDate>Sat, 23 Feb 2019 00:49:58 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/all-but-the-top/</guid>
      <description>概要 ICLR2018で発表されたAll-but-the-Top: Simple and Effective Postprocessing for Word Representationsの後処理を実装し、日本語学習済み分散表現に対して適用し評価を行いました。また、作成した単語ベクトルを配布します。
All-but-the-Top All-but-the-Topは、学習済みの分散表現に対して特定の後処理を加えることにより、分散表現の評価に用いられるタスクにおいて性能向上を達成した手法です。単語ベクトル内に存在する偏りを無くすために、平均で標準化し、主成分分析で幾つかの方向の主成分を引くという処理をするというのものです。たったこれだけという感じですが、SIF Embeddingの研究と同様に理論的な裏付けがあるようです。こういった背景や英語での実験結果は論文を参考ください。日本語での解説はこちらの論文紹介スライドが参考になります。
単語ベクトルのダウンロード 以下の2つの学習済み分散表現に対してAll-but-the-Topの後処理を適用したファイルです。配布するモデルは、元のファイル名に加えてabttという名前が付与されています。
   ファイル 次元数/語彙数 ファイルサイズ     jawiki.word_vectors.100d.abtt.bin 100 / 727,471 285M   jawiki.word_vectors.200d.abtt.bin 200 / 727,471 563M   jawiki.word_vectors.300d.abtt.bin 300 / 727,471 840M   cc.ja.300d.abtt.bin 300 / 2,000,000 2.3G     jawiki.word_vectors: 日本語 Wikipedia エンティティベクトル  20181001版のjawiki.word_vectorsを使用 鈴木正敏, 松田耕史, 関根聡, 岡崎直観, 乾健太郎. Wikipedia 記事に対する拡張固有表現ラベルの多重付与. 言語処理学会第22回年次大会(NLP2016), March 2016.</description>
    </item>
    
    <item>
      <title>語彙を限定して単語ベクトルのモデルサイズを小さくするminify_w2v</title>
      <link>https://yag-ays.github.io/project/minify-w2v/</link>
      <pubDate>Tue, 19 Feb 2019 10:03:37 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/minify-w2v/</guid>
      <description>概要 最近では単語埋め込みのベクトルをニューラルネットの埋め込み層に利用する手法が多く使われていますが、学習済みの単語埋め込みは多くの語彙を含んでおり、ファイルサイズが大きくロード時間もかかります。再現性のために学習に使う外部データを固定したり、APIのためにDockerコンテナに同梱する際には、こういったリソースはなるべくサイズを減らしたいという場合があります。そこで、必要な語彙に限定することで学習済み単語埋め込みのモデルサイズを小さくするコードを書きました。
yagays/minify_w2v: Minify word2vec model file
使い方 動作は至ってシンプルで、読み込んだ学習済み単語ベクトルの中から指定された単語のベクトルのみを抜き出して出力するだけです。
 学習済みモデルを読み込む 必要な語彙を指定する 出力する  load_word2vecとsave_word2vecにはそれぞれbinaryオプションがあり、入力および出力をバイナリフォーマットかテキストフォーマットか選択できます。
mw2v = MinifyW2V() mw2v.load_word2vec(&amp;quot;/path/to/model.txt&amp;quot;) mw2v.set_target_vocab(target_vocab=[&amp;quot;cat&amp;quot;, &amp;quot;dog&amp;quot;]) mw2v.save_word2vec(&amp;quot;/path/to/output.bin&amp;quot;, binary=True)  ベンチマーク 日本語 Wikipedia エンティティベクトルの単語ベクトルを元に、そこから語彙を10,000に減らしたときのファイルサイズや読み込み時間を比較した結果です。テキストファイルの場合はファイルフォーマットの構造上、語彙を減らせば線形でファイルサイズも小さくなりますし読み込み時間も小さくなります。
   Name Vocab. size File size Load time     jawiki.word_vectors.200d.txt 727,471 1.6G 1min 27s   jawiki.word_vectors.200d.bin 727,471 563M 5.9 s   jawiki.word_vectors.200d.10000.txt 10,000 22M 1.18 s   jawiki.word_vectors.200d.10000.bin 10,000 7.7M 190 ms    plasticityai/magnitudeとの関連 単語ベクトルを効率よく扱う方法として、magnitudeという軽量で遅延読み込み等をサポートしたパッケージがあります。Magnitude Formatsという形式で保存することで読み込み速度を大幅に短縮することができるほか、単語埋め込みで利用される類似度検索や、KerasやPytorchなど他のパッケージとのインターフェイスも提供されています。</description>
    </item>
    
    <item>
      <title>日本語Wikipediaで学習したdoc2vecモデル</title>
      <link>https://yag-ays.github.io/project/pretrained_doc2vec_wikipedia/</link>
      <pubDate>Tue, 22 Jan 2019 15:55:40 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/pretrained_doc2vec_wikipedia/</guid>
      <description>日本語Wikipediaを対象にdoc2vec学習させたモデルを作成したので、学習済みモデルとして公開します。
概要 doc2vecは2014年にQuoc LeとTomas Mikolovによって発表された文章の埋め込みの手法です。今更doc2vecかという感じではありますが、日本語のdoc2vecの学習済みモデルは探した限り容易に利用できるものがなかったこともあり、せっかくなので作成したモデルを配布します。
word2vecのような単語の分散表現においては学習済みモデルとして配布されたものを利用することが多いですが、文章の埋め込みに関しては対象とするドキュメント集合やそのドメインに特化した学習モデルを作成することが多い印象です。なので、学習済みモデルファイルの配布自体にそれほど意味があるわけではなさそうですが、既存手法との比較に利用したり、とりあえず何かしらの手法で単語列から文章ベクトルにしたいといった場合には便利かと思います。まあ何も無いよりかはマシかなという雰囲気です。今回の作成の経緯として、別の手法を実装する際にdoc2vecが内部で使われていたということで副次的に必要になったからだったのですが、ふと利用したいときに気軽に利用できるというのは結構良いのではないかと思います。
モデル ここでは2つの学習アルゴリズムでdoc2vecを学習しました。dbow300dはdistributed bag of words (PV-DBOW)を、dmpv300dはdistributed memory (PV-DM)を用いています。なお、モデルファイルはサイズが大きいため、Googleドライブに配置してあります。下記リンク先からダウンロードしてください。
 dbow300d  https://www.dropbox.com/s/j75s0eq4eeuyt5n/jawiki.doc2vec.dbow300d.tar.bz2?dl=0 圧縮ファイルのサイズ：5.48GB  dmpv300d  https://www.dropbox.com/s/njez3f1pjv9i9xj/jawiki.doc2vec.dmpv300d.tar.bz2?dl=0 圧縮ファイルのサイズ：8.86GB   モデルの学習パラメータ    param dbow300d dmpv300d     dm 0 1   vector_size 300 300   window 15 10   alpha 0.025 0.05   min_count 5 2   sample 1e-5 0   epochs 20 20   dbow_words 1 0    dbow300dのパラメータは、An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding GenerationにおけるEnglish Wikipeiaの学習時のパラメータを利用しました。dmpv300dのパラメータは、Gensim Doc2Vec Tutorial on the IMDB Sentiment Datasetの設定を参考にしました。</description>
    </item>
    
    <item>
      <title>Wikipediaの記事ごとのページビューを取得する</title>
      <link>https://yag-ays.github.io/project/wikipedia-pageview/</link>
      <pubDate>Sun, 16 Dec 2018 12:42:50 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/wikipedia-pageview/</guid>
      <description>自然言語処理においてWikipediaのテキストコーパスは広く利用されており、各記事のページビュー(閲覧数)の情報もトレンド分析やエンティティリンキング等で用いられています。この記事では、Wikipediaの記事ごとのページビューを取得する方法を紹介します。
tl;dr  ウェブから簡単に調べるなら → Pageviews Analysis 少数の記事についてプログラムから利用したいなら → Pageview API 大量の記事についてプログラムから利用したいなら → Wikimedia Analytics Datasets  1. Pageviews Analysisを利用する 手軽にページビューを確認するには「Pageviews Analysis」というウェブサイトがもっとも容易です。
Pageviews Analysisではグラフによる時系列の可視化、複数記事の比較、編集履歴の回数、csvやjsonによるダウンロードなど、多様な機能を備えています。また現在どのようなページが多く閲覧されているかといった言語ごとの閲覧数ランキングなどの機能もあり、とりあえず何か調べるなら大抵のことはPageviews Analysisで完結すると思います。
ちなみに日本語の記事を検索する場合は「プロジェクト」をja.wikipedia.orgに指定するのを忘れずに。
2. Pageview APIを利用する Wikimediaにはページビューを取得するREST APIが用意されています。指定できるパラメータや得られる情報ははPageviews Analysisと大差ありませんが、json形式で取得できるのでプログラムへの連携が簡単になります。幾つかリストアップした記事ごとに手軽にページビューを得たいという場合には最適な方法です。ただし100リクエスト/秒という制限があるので、日本語の記事全部のページビューを得たいといった用途には不向きです。
Wikimedia REST APIドキュメント
こちらもREST APIのドキュメントからパラメータを指定してリクエストを送り、ウェブ上で結果を確認する機能があります。同時にcurlのコマンドやURLのエンドポイントも自動で生成してくれるので、サンプルで動かす際には便利です。
例えばプロジェクトja.wikipedia.orgにおける機械学習という記事の2018/12/01から2018/12/03ページビューを得るときのコマンドは以下のようになりました。
$ curl -X GET --header &#39;Accept: application/json; charset=utf-8&#39; &#39;https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ja.wikipedia.org/all-access/all-agents/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86/daily/20181201/20181203&#39;  返ってくる結果は以下のようになります。
{ &amp;quot;items&amp;quot;: [ { &amp;quot;project&amp;quot;: &amp;quot;ja.wikipedia&amp;quot;, &amp;quot;article&amp;quot;: &amp;quot;機械学習&amp;quot;, &amp;quot;granularity&amp;quot;: &amp;quot;daily&amp;quot;, &amp;quot;timestamp&amp;quot;: &amp;quot;2018120100&amp;quot;, &amp;quot;access&amp;quot;: &amp;quot;all-access&amp;quot;, &amp;quot;agent&amp;quot;: &amp;quot;all-agents&amp;quot;, &amp;quot;views&amp;quot;: 210 }, [...] ] }  なお、ドキュメントにも記載されていますが、複数のクエリを機械的にリクエストするときにはUser-AgentやApi-User-Agentをヘッダに含めてリクエスト送り元がわかるようにしましょう。Pythonのrequestsパッケージで実行する場合は以下のようにヘッダで情報を付与します。</description>
    </item>
    
    <item>
      <title>A La Carte Embeddingの実装</title>
      <link>https://yag-ays.github.io/project/alacarte/</link>
      <pubDate>Fri, 07 Dec 2018 08:56:02 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/alacarte/</guid>
      <description>ACL2018にて発表された&amp;ldquo;A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors&amp;rdquo;を実装しました。未知語やngramなどの単語埋め込みを既知の学習済みベクトルから計算する手法です。
この記事はSansan Advent Calendar 2018 の8日目の記事です。
概要 &amp;ldquo;A La Carte Embedding&amp;rdquo;は、文脈における周囲の単語埋め込みを平均したものが学習済みの単語埋め込みと一致するように線形変換を学習することで、未知語に関しても単語埋め込みのベクトルを推定する手法です。これにより、通常の単語埋め込みでは学習が難しいような低頻度語であったり、複合名詞などの複数の単語からなる語においても、分散表現を得ることができます。
本論文の著者らは、これまでにSIF Embeddingなど理論寄りな方面から単語埋め込みや文章のベクトル表現を研究しているプリンストン大学のSanjeev Aroraのグループです。
ロジック A La Carte Embeddingに必要なのは、大規模コーパスと学習済みの単語ベクトルです。
学習の流れとしては、
 単語埋め込みを計算したい低頻度語やngramなどの語彙を用意する 学習済みの単語埋め込みの語彙と1.で用意した語彙を対象に、学習コーパス内で文脈内に存在する単語埋め込みの和や出現数から文脈における埋め込みを得る これらのうち学習済みの単語埋め込みに関しては正解がわかっているので、2.で作成した文脈における埋め込みのがこの正解と一致するような線形変換を学習する 学習した線形変換を利用して、2.で計算した低頻度語やngramなどのベクトル表現に関しても線形変換を行って単語埋め込みを得る  という感じです。
例えば(&amp;quot;単語&amp;quot;,&amp;quot;埋め込み&amp;quot;)というbigramのベクトル表現を得たいと思ったとしたら、コーパス内でこのbigramの周囲の単語埋め込みのベクトルを足し合わせてその出現頻度で割ることで、2.の文脈埋め込みが得られます。なお、ここで言う文脈はword2vecと同じく、特定のウィンドウ幅に含まれる単語の集合のようなものです。それをコーパス内で先頭から順に、すべての語彙で行います。線形変換の学習には、既知の単語埋め込みにある単語ベクトル（上図でいう吾輩のような既知の単語など）と、今回作成した文脈埋め込みが等しくなるように学習させます。あとはこの線形変換をbigramの文脈埋め込みに掛けることによって目的とするベクトル表現が得られます。
ソースコード yagays/alacarte_embedding: Python implementation of A La Carte Embedding
なお、本ソースコードで精度評価やオリジナル実装との比較は行っていません。バグや細部の実装の違いが含まれている場合がありますのでご注意ください。
オリジナルの実装 著者らがオリジナルの実装を公開しています。こちらは(当然ながら)スペースで単語が分割できる英語などの言語を対象にしています。
 NLPrinceton/ALaCarte  使い方 ALaCarteEmbeddingを作成し、コーパスに対して実行します。パラメータは以下の通りです。
 word2vec: 学習済み単語埋め込み  gensimのWord2VecKeyedVectorsを前提としています  tokenize: トークナイザ min_count: 対象とする単語のコーパス内での頻度の最小値 ngram: 対象とするngram  alc = ALaCarteEmbedding(word2vec=w2v, tokenize=tokenize, min_count=10, ngram=[1, 2]) alc.</description>
    </item>
    
    <item>
      <title>Word Embedding based Edit Distanceの実装</title>
      <link>https://yag-ays.github.io/project/wed/</link>
      <pubDate>Mon, 12 Nov 2018 21:54:12 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/wed/</guid>
      <description>Leading NLP Ninjaのep.12で紹介されていたWord Embedding based Edit Distanceを実装してみました。
Word Embedding based Edit Distance Word Embedding based Edit Distance（WED）は、文字列間の類似度を計算する編集距離（Edit Distance）を拡張して、単語埋め込みの類似度を使うことによって文章間の意味的な距離を編集距離的に計算しようというものです。編集距離では文字の追加/削除/置換のコストが1なのに対し、WEDではそれぞれのコストは以下のように設定しています。問題は単語埋め込みの類似度をどう使うかですが、追加と置換のコストに関しては対応する文中に近い単語があればそっちの類似度分をコスト1から差し引く、置換に関しては単語間の類似度を単純にコストから差し引くといった形で拡張しています。
ここで、単語間の類似度は以下のように計算しています。
なので、このアルゴリズムに出てくるパラメータはw,b,λ,μの4つです。
詳しくはLeading NLP Ninjaの解説の方を参照ください。
実装 yagays/wed: Python implementation of Word Embedding based Edit Distance
基本的には編集距離の実装に用いる動的計画法をそのままに、コスト部分を論文通りに拡張すればOKです。ただし、実装が合っているかどうかすごく不安なので、あくまで参考適度に……。
注意点として、DPの配列の0行目と0列目では片方の文字列を削除したときのコストが入るので、単純に行/列ごとに1をインクリメントしていくのではなく、上記コストを計算する必要があります。
評価 流石に論文のような評価はしていませんが、手元でSTAIR Captionsを対象に類似度計算をした結果、まあまあ妥当そうな結果になりました。ここでは15,000件くらいの文章を対象に、馬が走っているという文章に似た文章を近い順に10件列挙しています。
[[&#39;一頭の茶色い馬が走っている&#39;, 0.6571005980449359], [&#39;芝生の上を人がのった馬が走っている&#39;, 1.073464996815989], [&#39;黄色いスクールバスが走っている&#39;, 1.0986647573718564], [&#39;道路を首輪をした白い馬が走っている&#39;, 1.1281587567774887], [&#39;尻尾の長い茶色の馬が歩いている&#39;, 1.1786151622459524], [&#39;汽車のミニチュアが走っている&#39;, 1.2233853716174816], [&#39;車の外を白や茶色の馬が走っている&#39;, 1.2432452098145987], [&#39;大きな道路をトラックが走っている&#39;, 1.2456263623812807], [&#39;フリスビーをくわえた犬が走っている&#39;, 1.2533987344007116], [&#39;曲がりくねった道をレーサーが走っている&#39;, 1.2657003314558781]]  感想 実装/評価する上で気になった点を幾つか。
1. 追加／削除のコスト まずはじめに気になる点として、WEDの追加/削除のコスト計算がそれで良いのか問題。今回の論文の計算方法では、文章中の全単語の中から最も近い単語の類似度を用いるので、文中でどれだけ離れていても一番意味的に近い単語が採用される点が気になります。文章が長ければ長いほど似た単語が現れやすいでしょうし、編集距離を拡張している意味が何なのかよくわからなくなってきます。一度類似した単語として利用したら以降は使わないとか、前後n単語のみの類似度を使うとか、もう少し改良が必要な気がします。
2. パラメータ調整 そもそも最適なパラメータ調整がかなり難しいです。追加/削除と置換のコストがバランス取れていないと一単語置換するより1単語削除して1単語追加したほうが良いみたいになることもあり、この辺ピーキーすぎて調整が難しいところ。単語間類似度を計算するときにcos類似度→[0,1]の類似度に変換している部分も、改良の余地があると思います。
3. 文字列間の文章長の影響 また、極端に短い文章を対象にすると、意味的に関係が無くてもトータルのコストが低くなる場合があります。このあたりはコスト周りのパラメータの調整でも限界があると思うので、Quoraの質問ペアのようなある程度整ったデータセットでないと辛い部分でしょう。といってもそこは従来の編集距離でも同じことなので、的外れな意見ではあります。</description>
    </item>
    
    <item>
      <title>学習済み分散表現をTensorBoardで可視化する (gensim/PyTorch/tensorboardX)</title>
      <link>https://yag-ays.github.io/project/embedding-visualization/</link>
      <pubDate>Tue, 28 Aug 2018 21:37:26 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/embedding-visualization/</guid>
      <description>word2vecや系列モデル等で学習した分散表現の埋め込みベクトル（word embeddings）は、単語の意味をベクトル空間上で表現することが可能です。最も有名な例では「King - Man + Woman = Queen」のように意味としての加算や減算がベクトル計算で類推可能なこともあり、ベクトル空間の解釈として低次元へ写像する形で分散表現の可視化が行われています。
可視化の際に用いられるツールとしては、TensorFlowのツールの一つであるTensorBoardが、豊富な機能とインタラクティブな操作性を備えていて一番使い勝手が良いと思います。ただ、TensorFlowと組み合わせた可視化は容易なのですが、他のツールやパッケージで作成したコードをそのまま読み込めないなど、かゆいところに手が届かないと感じる部分もあります。
そこで今回は、すでに学習された単語の分散表現を可視化するために、
 gensimを用いてベクトルを読み込み、 PyTorchのTensor形式に変換したうえで、 tensorboardXを用いてTensorBoardが読み込めるログ形式に出力する  ことで、TensorBoard上で分散表現を可視化します。いろいろなステップがあって一見して遠回りに思えますが、コード自体は10行に満たないほどで完結します。個人的には、Tensorflowで学習済み分散表現を読み込むよりも、これらを組み合わせたやり方のほうが簡潔に書くことができて好きです。
方法 準備 必要な外部パッケージは、gensim/pytorch/tensorboardX/tensorflowの4つです。インストールされていない場合はpipなどであらかじめインストールしておきます。
$ pip isntall gensim torch tensorboardX tensorflow  分散表現の読み込みからtensorboard形式のログ出力まで TensorBoardで可視化するまでに必要な本体のコードです。これだけ！
import gensim import torch from tensorboardX import SummaryWriter vec_path = &amp;quot;entity_vector.model.bin&amp;quot; writer = SummaryWriter() model = gensim.models.KeyedVectors.load_word2vec_format(vec_path, binary=True) weights = model.vectors labels = model.index2word # DEBUG: visualize vectors up to 1000 weights = weights[:1000] labels = labels[:1000] writer.add_embedding(torch.FloatTensor(weights), metadata=labels)  コード内のvec_pathは、任意の学習済みベクトルのファイルに置き換えてください。</description>
    </item>
    
    <item>
      <title>📙Unicode絵文字の日本語読み/キーワード/分類辞書📙</title>
      <link>https://yag-ays.github.io/project/emoji-ja/</link>
      <pubDate>Thu, 23 Aug 2018 07:41:44 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/emoji-ja/</guid>
      <description>emoji_jaは、Unicodeに登録されている絵文字に対して、日本語の読みやキーワード、分類を付与したデータセットです。Unicodeで定められている名称やアノテーションを元に構築しています。
TwitterやInstagramなどのSNSを通じた絵文字の普及により、emoji2vecやdeepmojiなどの絵文字を使った自然言語処理の研究が行われるようになりました。絵文字を含む分析においては、絵文字の持つ豊富な情報や多彩な利用方法により、従来の形態素分析などのテキスト処理では対応できない場合があります。例えば、「今日は楽しかった😀」という文章では感情表現として絵文字が使われていますが、「今日は🍣を食べて🍺を飲んだ」ではそれぞれの対象を表す単語として用いられることもあります。[佐藤,2015]では絵文字の品詞を名詞/サ変名詞/動詞/副詞/記号/感動詞の6種類に分類しており、形態素解析に用いるNEologd辞書にも絵文字に対応する言葉が複数登録されています。
このように、絵文字を機械的な処理や研究対象として扱うには、絵文字の読み方であったり意味を表す単語、または意味的な種類で分類したカテゴリが必要になります。こうした辞書は、英語においてはemojilibがありますが、絵文字は文化的に異なった意味として用いられる場合があるため、それらの対訳をそのまま利用できないことがあります。
そのため、日本語で容易に使えるリソースとしてemoji_jaを作成しました。
💻 ダウンロード 以下のGitHubレポジトリからjson形式のファイルをダウンロードできます。data/配下にある各種jsonファイルが、データセットの本体です。
yagays/emoji-ja: 📙UNICODE絵文字の日本語読み/キーワード/分類辞書📙
📁 データセット emoji-jaには下記の3種類のデータが含まれています。
 emoji_ja.json: 絵文字に対応するキーワードやメタ情報 group2emoji_ja.json: 絵文字のグループ/サブグループに対応した絵文字のリスト keyword2emoji_ja.json: 絵文字のキーワードに対応した絵文字のリスト  1️⃣ emoji_ja.jsonデータ emoji_ja.jsonには、絵文字に対応する以下のメタデータが含まれています。
   カラム 概要 取得元     keywords 絵文字に関連したキーワード CJK Annotations (CLDR Version 33)   short_name 絵文字を表す短い名前 CJK Annotations (CLDR Version 33)   group 絵文字を意味的に分類したときのグループ Emoji List, v11.0を元に翻訳   subgroup 絵文字を意味的に分類したときのサブグループ Emoji List, v11.0を元に翻訳    { &amp;quot;♟&amp;quot;: { &amp;quot;keywords&amp;quot;: [ &amp;quot;チェス&amp;quot;, &amp;quot;チェスの駒&amp;quot;, &amp;quot;捨て駒&amp;quot;, &amp;quot;駒&amp;quot; ], &amp;quot;short_name&amp;quot;: &amp;quot;チェスの駒&amp;quot;, &amp;quot;group&amp;quot;: &amp;quot;活動&amp;quot;, &amp;quot;subgroup&amp;quot;: &amp;quot;ゲーム&amp;quot; }, &amp;quot;♾&amp;quot;: { &amp;quot;keywords&amp;quot;: [ &amp;quot;万物&amp;quot;, &amp;quot;永遠&amp;quot;, &amp;quot;無限&amp;quot;, &amp;quot;無限大&amp;quot; ], &amp;quot;short_name&amp;quot;: &amp;quot;無限大&amp;quot;, &amp;quot;group&amp;quot;: &amp;quot;記号&amp;quot;, &amp;quot;subgroup&amp;quot;: &amp;quot;その他 シンボル&amp;quot; }, .</description>
    </item>
    
    <item>
      <title>漢字を構成する部首/偏旁のデータセット</title>
      <link>https://yag-ays.github.io/project/kanjivg-radical/</link>
      <pubDate>Mon, 06 Aug 2018 08:43:23 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/kanjivg-radical/</guid>
      <description>kanjivg-radicalは、漢字を構成する部首や偏旁を容易に扱えるように対応付けしたデータセットです。
「脳」という漢字は、「月」「⺍」「凶」のように幾つかのまとまりごとに細分化できます。このように意味ある要素に分解しデータセットにすることで、漢字を文字的に分解して扱ったり、逆に特定の部首/偏旁を持つ漢字を一括して検索することができます。
このデータセットは、KanjiVGで公開されているsvgデータを抽出および加工して作成されています。そのため、本データセットに含まれる部首/偏旁のアノテーションはすべてKanjiVGに準拠します。
ダウンロード 以下のGitHubレポジトリからjson形式のファイルをダウンロードできます。data/配下にある各種jsonファイルが、データセットの本体です。
yagays/kanjivg-radical
データセットの詳細 kanjivg-radicalには4種類のデータが含まれています。
 漢字と部首/偏旁の変換データ 漢字と要素の変換データ 漢字と部首/偏旁の変換データのうち、左右に分割できる漢字 漢字と部首/偏旁の変換データのうち、上下に分割できる漢字  以下では、部首/偏旁はradical、要素はelement、左右はleft_right、上下はtop_buttomと表現しています。
1. 漢字と部首/偏旁の変換データ 漢字と部首/偏旁を対応付けしたデータセットです。漢字から部首/偏旁と、部首/偏旁から漢字の2種類のデータがあります。
 kanji2radical.json : 漢字から部首/偏旁への変換 radical2kanji.json : 部首/偏旁から漢字への変換  # kanji2radical.jsonのサンプル &amp;quot;脳&amp;quot;: [ &amp;quot;月&amp;quot;, &amp;quot;⺍&amp;quot;, &amp;quot;凶&amp;quot; ]  # radical2kanji.jsonのサンプル &amp;quot;月&amp;quot;: [ &amp;quot;肝&amp;quot;, &amp;quot;育&amp;quot;, &amp;quot;胆&amp;quot;, &amp;quot;朦&amp;quot;, &amp;quot;脱&amp;quot;, ...  2. 漢字と要素の変換データ 漢字と要素を対応付けしたデータセットです。漢字から要素と、要素から漢字の2種類のデータがあります。
 kanji2element.json : 漢字から要素への変換 element2kanji.json : 要素から漢字への変換  ここで使用している「要素」いう言葉は、部首/偏旁を構成する更に細かい単位での漢字のことを指しています。言語学的に定義された単語ではなく、KanjiVGで利用されていたelementの対訳として用いています。
このデータでは構成している要素をすべて列挙しているので、結果の中には重複が含まれます。以下の例だと脳には「凶」という要素が含まれていますが、同時に「乂」という要素も含まれているため、どちらもkanji2elementの結果として出力されます。
# kanji2element.jsonのサンプル &amp;quot;脳&amp;quot;: [ &amp;quot;乂&amp;quot;, &amp;quot;凶&amp;quot;, &amp;quot;丿&amp;quot;, &amp;quot;凵&amp;quot;, &amp;quot;⺍&amp;quot;, &amp;quot;月&amp;quot; ]  # element2kanji.</description>
    </item>
    
    <item>
      <title>Wikipedia CirrusSearchのダンプデータを利用する</title>
      <link>https://yag-ays.github.io/project/cirrus/</link>
      <pubDate>Mon, 30 Jul 2018 21:07:52 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/cirrus/</guid>
      <description>Wikipediaのデータを容易に利用できるCirrusSearchのダンプデータについて紹介します。これを利用することにより、Wikipediaの巨大なXMLデータをパースしたり、Wikipedia Extractorなど既存のツールで前処理することなく、直にWikipediaの各種データにアクセスすることができます。
tl;dr 細かいことは置いておいて、素直にWikipediaの日本語エントリーに書かれているテキストを取得したい場合、
 ここのCirrusSearchの任意の日付のダンプデータjawiki-YYYYMMDD-cirrussearch-content.json.gzを落としてくる 中に入っているjsonデータをパースして、偶数行の&amp;quot;text&amp;quot;を取得するコードを書く  とすることで、簡単にWikipediaのテキストデータを取得することができます。
CirrusSearchダンプデータの概要 CirrusSearchは、ElasticSearchをバックエンドに構成された検索システムです。このシステムに利用されているデータがダンプデータとして公開されており、そのファイルを利用することで、整形されたテキストを始めとして、外部リンクのリストやカテゴリのリスト等のメタデータが容易に利用できます。また、言語ごとにダンプファイルが分かれているため、日本語のWikipediaのデータだけを対象にすることが可能です。
CirrusSearchのダンプデータは以下から取得します。
Index of /other/cirrussearch/
Wikipediaに関するダンプデータは以下の2つです。
   ファイル 内容     jawiki-YYYYMMDD-cirrussearch-content.json.gz Wikipediaの本文（namespaceが0）   jawiki-YYYYMMDD-cirrussearch-general.json.gz Wikipediaのその他情報（namespaceが0以外）    その他の接頭辞の対応関係は以下の通りです。
 jawiki: ウィキペディア jawikibooks: ウィキブックス jawikinews: ウィキニュース jawikiquote: ウィキクォート jawikisource: ウィキソース jawikiversity: ウィキバーシティ  CirrusSearchのデータ CirrusSearchのダンプデータは、1行が1つのjsonとなっており、2行で1つのエントリーを表しています。
奇数行 奇数行にはエントリーに固有のidが記載されています。この_idから該当のエントリーにアクセスするには、https://ja.wikipedia.org/?curid=3240437のようにcuridのパラーメータを指定します。
{ &amp;quot;index&amp;quot;: { &amp;quot;_type&amp;quot;: &amp;quot;page&amp;quot;, &amp;quot;_id&amp;quot;: &amp;quot;3240437&amp;quot; } }  偶数行 偶数行にはエントリーの情報が記載されています。下記の例では、複数の要素が入った配列や長い文字列は...で省略しています。
{ &amp;quot;template&amp;quot;: [ &amp;quot;Template:各年の文学ヘッダ&amp;quot;, .</description>
    </item>
    
    <item>
      <title>Home</title>
      <link>https://yag-ays.github.io/home/</link>
      <pubDate>Thu, 26 Jul 2018 04:28:29 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/home/</guid>
      <description>yag_aysの資材置き場。out-of-the-boxなデータセット/コーパス/ノウハウを公開していきたい。
記事一覧はこちら→ PROJECT
プロフィール→ Yuki Okuda</description>
    </item>
    
    <item>
      <title>文字の図形的な埋め込み表現 Glyph-aware Character Embedding</title>
      <link>https://yag-ays.github.io/project/char-embedding/</link>
      <pubDate>Wed, 25 Jul 2018 12:30:41 +0900</pubDate>
      
      <guid>https://yag-ays.github.io/project/char-embedding/</guid>
      <description>「文字の図形的な埋め込み表現」は、文字の図形的な情報から埋め込み表現を学習したデータセットです。文字の意味や文章中の文脈などのセマンティクスから構成する分散表現とは違い、文字の形状という視覚的な特徴を学習しています。それぞれの文字に対する埋め込み表現の近さを計算することで、似た形の文字を推定することができます。
ダウンロード 下記のGitHubレポジトリからダウンロード可能です。以下のURLを開いて「Download」をクリックしてください。
convolutional_AE_300.tar.bz2 (解凍前:88MB, 解凍後:180MB)
以下の2つのファイルが入っています。フォーマットが異なるだけで、どちらも同じベクトルデータです。
 convolutional_AE_300.bin convolutional_AE_300.txt  その他サンプルコードなどのすべてのファイルは、以下のレポジトリにあります。
yagays/glyph-aware-character-embedding
詳細  ベクトル次元：300 文字の種類数：44,637 学習データに用いたフォント：Google Noto Fonts NotoSansCJKjp-Regular  使い方 gensimを用いた利用方法を例示します。なお、ここではword2vecのように単語の分散表現として扱っていますが、本リソースで扱う文字の図形的な埋め込み表現には加法性がありません。図形としての文字の類似度は計算できますが、部首の足し算引き算といったような操作はできないのでご注意下さい。
from gensim.models import KeyedVectors model = KeyedVectors.load_word2vec_format(&amp;quot;data/convolutional_AE_300.bin&amp;quot;, binary=True)  most_similar()を用いて、図形的な類似文字を検索します。以下の例では一番類似度が高い文字に「а」が来ていますが、これはasciiの「a」ではなくキリル文字の「a」です。
In []: model.most_similar(&amp;quot;a&amp;quot;) Out[]: [(&#39;а&#39;, 1.0000001192092896), (&#39;ả&#39;, 0.961397111415863), (&#39;ä&#39;, 0.9610118269920349), (&#39;ā&#39;, 0.9582812190055847), (&#39;á&#39;, 0.957198441028595), (&#39;à&#39;, 0.9558833241462708), (&#39;å&#39;, 0.938391923904419), (&#39;ầ&#39;, 0.9370290040969849), (&#39;ǎ&#39;, 0.9368112087249756), (&#39;ấ&#39;, 0.9365179538726807)]  Google Noto Fonts NotoSansCJKjp-Regularに含まれるすべての文字に対して操作が可能です。
In []: model.most_similar(&amp;quot;油&amp;quot;) Out[]: [(&#39;汕&#39;, 0.9025427103042603), (&#39;泊&#39;, 0.</description>
    </item>
    
  </channel>
</rss>